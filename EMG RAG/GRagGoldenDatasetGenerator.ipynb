{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CITIES INFORMATIONS\n",
    "This notebook is used to show how Graph RAG performs when structured and unstructured data are both used in the logic.\n",
    "It will be showed :\n",
    "  - how to ingest different kind of data (online PDF files, tabular CSV data, ecc...)\n",
    "  - how to instantiate the LLM and Langchain\n",
    "  - how to connect to a Neo4j instance in order to show the generated graph\n",
    "  - how to query the Neo4j graph\n",
    "  - how to use the prompts to query the LLM\n",
    "\n",
    "## STEP 0 - Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "## Imports for Ditto, the Entity Resolution system\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import deepmatcher as dm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:09:38.417727500Z",
     "start_time": "2024-10-16T14:09:28.425810900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "NEO4J_URI = os.environ[\"NEO4J_URI\"]\n",
    "NEO4J_USERNAME = os.environ[\"NEO4J_USERNAME\"]\n",
    "NEO4J_PASSWORD = os.environ[\"NEO4J_PASSWORD\"]\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:09:38.445729400Z",
     "start_time": "2024-10-16T14:09:38.425725900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "## STEP 1 - Documents and table ingestion\n",
    "In this phase, it is necessary to provide the dataset used to feed the Knowledge graph. To do this, we will ingest some PDF files containing informations about cities' air pollution and a table with all the data of most of the cities in the world."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1.1 - Documents ingestion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "urls = [\n",
    "    #\"https://www.iqair.com/dl/2023_World_Air_Quality_Report.pdf\",\n",
    "    \"https://www.istat.it/en/files/2011/01/qualita_aria_EN.pdf?title=Air+quality+in+European+cities+-+22+Jun+2010+-+qualita_aria_EN.pdf\",\n",
    "\n",
    "]\n",
    "\n",
    "csvFilePath = \"./worldcities.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:09:38.456726800Z",
     "start_time": "2024-10-16T14:09:38.445729400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"}\n",
    "docs = [PyPDFLoader(url, headers=headers).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:09:40.708437400Z",
     "start_time": "2024-10-16T14:09:38.454727400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1.2 - Documents' text splitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=7500, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:09:41.418945100Z",
     "start_time": "2024-10-16T14:09:40.708437400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1.3 - Table\n",
    "This phase will be posticipated to the **\"STEP 3 - Neo4j and graph insertion\"** since it is necessary to upload the **worldcities.csv** file directly into the graph DB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "## STEP 2 - Large Language Model and Graph generation from Documents\n",
    "This is one of the most important steps to cover: here we are going to instantiate the LLM (Large Language Model) used to extract the Entity and the Relationships from the documents in order to obtain the Nodes (entities) and Edges (relationships) of the Graph used in GraphRAG."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    model_name=\"llama3-70b-8192\")\n",
    "llm_transformer=LLMGraphTransformer(llm=llm)\n",
    "graph_documents=llm_transformer.convert_to_graph_documents(doc_splits)\n",
    "# graph_documents"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "## STEP 3 - Neo4j and graph insertion\n",
    "Now it is time to save the originated graph into a persistent Neo4j instance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3.1 - Initialize Neo4j connection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "database_name = \"progettotesi\"\n",
    "\n",
    "graph=Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=database_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:09:47.921565600Z",
     "start_time": "2024-10-16T14:09:41.418945100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CSV loading has been performed manually to save time\n",
    "# Step to execute to do it manually:\n",
    "#   - add the file \"worldcities.csv\" inside the Neo4j project's \"import\" folder\n",
    "#   - go to the folder \"bin\" inside the Neo4j project's folder\n",
    "#   - use the following command inside Neo4j terminal:\n",
    "#\n",
    "#     neo4j-admin database import full progettotesi --delimiter=\";\" --array-delimiter=\"U+007C\" --nodes=import/worldcities.csv\n",
    "\n",
    "\n",
    "# graph.query(\n",
    "#     query = \"LOAD CSV WITH HEADERS \"\n",
    "#     \"FROM 'file:///C:/Users/Gabri/OneDrive/Documenti/Universit%C3%A0/Tesi/RAG/RAG%20terzo/worldcities.csv' as row \"\n",
    "#     \"MERGE(\"\n",
    "#         \"m:City{\"\n",
    "#             \"id: row.city_ascii, \"\n",
    "#             \"latitude: row.lat, \"\n",
    "#             \"longitude: row.lng, \"\n",
    "#             \"country: row.country, \"\n",
    "#             \"iso2: row.iso2, \"\n",
    "#             \"iso3: row.iso3, \"\n",
    "#             \"administrative_name: row.admin_name, \"\n",
    "#             \"capital: row.capital, \"\n",
    "#             \"population: row.population\"\n",
    "#         \"}\"\n",
    "#     \")\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "# # Add etities (nodes) and relationships (edges) into the graph\n",
    "nodes_as_dict = flatten([list({'id': node.id, 'type': node.type} for node in doc.nodes) for doc in graph_documents])\n",
    "\n",
    "edges_as_dict = flatten([list(\n",
    "    {\n",
    "        'type': rel.type,\n",
    "        'source':{\n",
    "            'id': rel.source.id,\n",
    "            'type': rel.source.type\n",
    "         },\n",
    "        'target': {\n",
    "            'id': rel.target.id,\n",
    "            'type': rel.target.type\n",
    "        }\n",
    "    } for rel in doc.relationships) for doc in graph_documents])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for node in nodes_as_dict:\n",
    "    node_type = re.sub('[^A-Za-z0-9]+', '', node[\"type\"])\n",
    "\n",
    "    query = f\"\"\"\n",
    "    MERGE (n:{node_type} {\"{city_ascii: $id}\" if node['type'] == \"City\" else \"{id: $id}\"})\n",
    "    SET n.type = $type, n.updated = True\n",
    "    \"\"\"\n",
    "    graph.query(query=query, params={\"id\": node[\"id\"], \"type\": node[\"type\"]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for relationship in edges_as_dict:\n",
    "    source_id = relationship[\"source\"][\"id\"]\n",
    "    source_type = re.sub('[^A-Za-z0-9]+', '', relationship[\"source\"][\"type\"])\n",
    "\n",
    "    target_id = relationship[\"target\"][\"id\"]\n",
    "    target_type = re.sub('[^A-Za-z0-9]+', '', relationship[\"target\"][\"type\"])\n",
    "\n",
    "    rel_type = re.sub('[^A-Za-z0-9]+', '', relationship[\"type\"])\n",
    "\n",
    "    print(source_id + \"(\" + source_type + \")\" + \" -[\" + relationship[\"type\"] + \"]-> \" + target_id + \"(\" + target_type + \")\")\n",
    "\n",
    "    condition = \"(a.type = \\\"City\\\" and a.country IS NOT NULL and b.type = \\\"Country\\\" and a.country <> b.id) or (a.type = \\\"Country\\\" and b.type = \\\"City\\\" and b.country IS NOT NULL and a.id <> b.country)\"\n",
    "    query = f\"\"\"\n",
    "        MATCH (a:{source_type} {\"{city_ascii: $source_id}\" if source_type == \"City\" else \"{id: $source_id}\"})\n",
    "        MATCH (b:{target_type} {\"{city_ascii: $target_id}\" if target_type == \"City\" else \"{id: $target_id}\"})\n",
    "        CALL apoc.do.when(\n",
    "            {condition},\n",
    "            'RETURN null',\n",
    "            'MERGE (a)-[r:{rel_type}]->(b) return a, r, b',\n",
    "            {{a: a, b: b}}\n",
    "        )\n",
    "        YIELD value\n",
    "        return value\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    graph.query(\n",
    "        query=query,\n",
    "        params={\n",
    "            \"source_id\": source_id,\n",
    "            \"target_id\": target_id\n",
    "        }\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "## STEP 4 - Entity matching\n",
    "Since some of the nodes and relationships created are duplicated or have different names to point to the same concept, it is required to execute an \"Entity matching\" phase to reconciliate entities and make them back to just one.\n",
    "\n",
    "For this goal, we'll use **Ditto**, a deep learning-based Entity Matching system that leverages pre-trained language models (like BERT) to improve accuracy in identifying and matching similar entities across different datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4.1 - Prepare dataframe for Deepmatcher"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# citiesQueryResult = graph.query(query = \"MATCH (n:City) RETURN n\")\n",
    "# citiesDf = [item['n'] for item in citiesQueryResult]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PROFILE\n",
    "# MATCH (a:City)\n",
    "# WITH COLLECT(a) AS cities\n",
    "# UNWIND cities AS a\n",
    "# UNWIND cities AS b\n",
    "# WITH\n",
    "#     a,\n",
    "#     b,\n",
    "#     apoc.text.levenshteinSimilarity(a.city_ascii, b.city_ascii) AS city_ascii_sim,\n",
    "#     apoc.text.levenshteinSimilarity(a.country, b.country) AS country_sim\n",
    "# return a.city_ascii, a.country, b.city_ascii, b.country, city_ascii_sim, country_sim\n",
    "# LIMIT 1000000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "goldenDataset = pd.read_csv(\"neo4jCitiesGoldenDataset.csv\", sep=\";\", index_col=False)\n",
    "train_val_set, test_set = train_test_split(goldenDataset, test_size=0.2, random_state=42)\n",
    "train_set, val_set = train_test_split(train_val_set, test_size=0.25, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:20:53.535825Z",
     "start_time": "2024-10-16T14:20:51.429757800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_set.to_csv(\".\\\\dataset\\\\train_set.csv\", sep=\",\", index=True, index_label=\"id\")\n",
    "val_set.to_csv(\".\\\\dataset\\\\val_set.csv\", sep=\",\", index=True, index_label=\"id\")\n",
    "test_set.to_csv(\".\\\\dataset\\\\test_set.csv\", sep=\",\", index=True, index_label=\"id\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:20:58.917358300Z",
     "start_time": "2024-10-16T14:20:53.539824200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deepmatcher.data.dataset:Rebuilding data cache because: ['One or more data files have been modified.']\n",
      "\n",
      "Reading and processing data from \"dataset\\train_set.csv\"\n",
      "0% [###                           ] 100% | ETA: 00:03:47\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = \\\n",
    "    dm.data.process(\n",
    "        path='dataset',\n",
    "        train=\"train_set.csv\",\n",
    "        validation=\"val_set.csv\",\n",
    "        test=\"test_set.csv\",\n",
    "        use_magellan_convention=False,\n",
    "        label_attr='is_matching',\n",
    "        left_prefix=\"a.\",\n",
    "        right_prefix=\"b.\",\n",
    "        ignore_columns=['city_ascii_sim', 'country_sim'],\n",
    "        embeddings_cache_path=\".vector_cache\",\n",
    "        embeddings='fasttext.en.bin'\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:21:29.130384500Z",
     "start_time": "2024-10-16T14:20:58.907018700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "      id a.city_ascii      a.country b.city_ascii       b.country  is_matching\n0    876        cairo          egypt      cardito           italy            0\n1    326     shanghai          china      shanwei           china            0\n2    381     new york  united states      newport   united states            0\n3    853       manila    philippines      maniago           italy            0\n4    311       manila    philippines     marialva          brazil            0\n..   ...          ...            ...          ...             ...          ...\n595  118      karachi       pakistan      dalachi           china            0\n596  334     shanghai          china       sangli           india            0\n597  409     shenzhen          china     shepshed  united kingdom            0\n598  225       manila    philippines        laila           india            0\n599  482        delhi          india        delta   united states            0\n\n[600 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>a.city_ascii</th>\n      <th>a.country</th>\n      <th>b.city_ascii</th>\n      <th>b.country</th>\n      <th>is_matching</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>876</td>\n      <td>cairo</td>\n      <td>egypt</td>\n      <td>cardito</td>\n      <td>italy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>326</td>\n      <td>shanghai</td>\n      <td>china</td>\n      <td>shanwei</td>\n      <td>china</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>381</td>\n      <td>new york</td>\n      <td>united states</td>\n      <td>newport</td>\n      <td>united states</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>853</td>\n      <td>manila</td>\n      <td>philippines</td>\n      <td>maniago</td>\n      <td>italy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>311</td>\n      <td>manila</td>\n      <td>philippines</td>\n      <td>marialva</td>\n      <td>brazil</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>118</td>\n      <td>karachi</td>\n      <td>pakistan</td>\n      <td>dalachi</td>\n      <td>china</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>334</td>\n      <td>shanghai</td>\n      <td>china</td>\n      <td>sangli</td>\n      <td>india</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>597</th>\n      <td>409</td>\n      <td>shenzhen</td>\n      <td>china</td>\n      <td>shepshed</td>\n      <td>united kingdom</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>598</th>\n      <td>225</td>\n      <td>manila</td>\n      <td>philippines</td>\n      <td>laila</td>\n      <td>india</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>599</th>\n      <td>482</td>\n      <td>delhi</td>\n      <td>india</td>\n      <td>delta</td>\n      <td>united states</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>600 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table = train.get_raw_table()\n",
    "train_table"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-16T14:20:38.097536100Z",
     "start_time": "2024-10-16T14:20:38.053924800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 - Prepare data from Neo4j"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def base_serialization_logic(row, outfile):\n",
    "    serialized_row = \"\"\n",
    "    for column, value in row.items():\n",
    "        if(column != \"type\" and column != \"updated\"):\n",
    "            serialized_row += f\"COL {column} VAL {value} \"\n",
    "    serialized_row = serialized_row.strip()  # Rimuove lo spazio extra finale\n",
    "    outfile.write(serialized_row + \"\\n\")\n",
    "\n",
    "def serialize_csv(input_file, output_file):\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for row in reader:\n",
    "                base_serialization_logic(row, outfile)\n",
    "\n",
    "def serialize_dataframe(df, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for index, row in df.iterrows():\n",
    "            base_serialization_logic(row, outfile)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_file = 'serialized_cities.txt' # Nome del file di output serializzato\n",
    "serialize_dataframe(pd.DataFrame(citiesDf), output_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
