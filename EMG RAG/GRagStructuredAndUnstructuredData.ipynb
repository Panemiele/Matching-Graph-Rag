{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CITIES INFORMATIONS\n",
    "This notebook is used to show how Graph RAG performs when structured and unstructured data are both used in the logic.\n",
    "It will be showed :\n",
    "  - how to ingest different kind of data (online PDF files, tabular CSV data, ecc...)\n",
    "  - how to instantiate the LLM and Langchain\n",
    "  - how to connect to a Neo4j instance in order to show the generated graph\n",
    "  - how to query the Neo4j graph\n",
    "  - how to use the prompts to query the LLM\n",
    "\n",
    "## STEP 0 - Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_groq import ChatGroq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "## Imports for Deepmatcher, the Entity Resolution system\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import deepmatcher as dm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:01:42.974778200Z",
     "start_time": "2024-10-24T19:01:37.938258100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "NEO4J_URI = os.environ[\"NEO4J_URI\"]\n",
    "NEO4J_USERNAME = os.environ[\"NEO4J_USERNAME\"]\n",
    "NEO4J_PASSWORD = os.environ[\"NEO4J_PASSWORD\"]\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:01:43.007078100Z",
     "start_time": "2024-10-24T19:01:42.974778200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "## STEP 1 - Documents and table ingestion\n",
    "In this phase, it is necessary to provide the dataset used to feed the Knowledge graph. To do this, we will ingest some PDF files containing informations about cities' air pollution and a table with all the data of most of the cities in the world."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1.1 - Documents ingestion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "urls = [\n",
    "    #\"https://www.iqair.com/dl/2023_World_Air_Quality_Report.pdf\",\n",
    "    \"https://www.istat.it/en/files/2011/01/qualita_aria_EN.pdf?title=Air+quality+in+European+cities+-+22+Jun+2010+-+qualita_aria_EN.pdf\",\n",
    "\n",
    "]\n",
    "\n",
    "csvFilePath = \"./worldcities.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:01:43.013347600Z",
     "start_time": "2024-10-24T19:01:43.007078100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"}\n",
    "docs = [PyPDFLoader(url, headers=headers).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:01:44.417664900Z",
     "start_time": "2024-10-24T19:01:43.007785900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1.2 - Documents' text splitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=7500, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:01:44.757845400Z",
     "start_time": "2024-10-24T19:01:44.417664900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1.3 - Table\n",
    "This phase will be posticipated to the **\"STEP 3 - Neo4j and graph insertion\"** since it is necessary to upload the **worldcities.csv** file directly into the graph DB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "## STEP 2 - Large Language Model and Graph generation from Documents\n",
    "This is one of the most important steps to cover: here we are going to instantiate the LLM (Large Language Model) used to extract the Entity and the Relationships from the documents in order to obtain the Nodes (entities) and Edges (relationships) of the Graph used in GraphRAG."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    model_name=\"llama3-70b-8192\")\n",
    "llm_transformer=LLMGraphTransformer(llm=llm)\n",
    "graph_documents=llm_transformer.convert_to_graph_documents(doc_splits)\n",
    "# graph_documents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T13:28:01.987100100Z",
     "start_time": "2024-10-23T13:28:01.963075600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "## STEP 3 - Neo4j and graph insertion\n",
    "Now it is time to save the originated graph into a persistent Neo4j instance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3.1 - Initialize Neo4j connection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "database_name = \"progettotesi\"\n",
    "\n",
    "graph=Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=database_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:01:51.714445300Z",
     "start_time": "2024-10-24T19:01:44.759290800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CSV loading has been performed manually to save time\n",
    "# Step to execute to do it manually:\n",
    "#   - add the file \"worldcities.csv\" inside the Neo4j project's \"import\" folder\n",
    "#   - go to the folder \"bin\" inside the Neo4j project's folder\n",
    "#   - use the following command inside Neo4j terminal:\n",
    "#\n",
    "#     neo4j-admin database import full progettotesi --delimiter=\";\" --array-delimiter=\"U+007C\" --nodes=import/worldcities.csv\n",
    "\n",
    "\n",
    "# graph.query(\n",
    "#     query = \"LOAD CSV WITH HEADERS \"\n",
    "#     \"FROM 'file:///C:/Users/Gabri/OneDrive/Documenti/Universit%C3%A0/Tesi/RAG/RAG%20terzo/worldcities.csv' as row \"\n",
    "#     \"MERGE(\"\n",
    "#         \"m:City{\"\n",
    "#             \"id: row.city_ascii, \"\n",
    "#             \"latitude: row.lat, \"\n",
    "#             \"longitude: row.lng, \"\n",
    "#             \"country: row.country, \"\n",
    "#             \"iso2: row.iso2, \"\n",
    "#             \"iso3: row.iso3, \"\n",
    "#             \"administrative_name: row.admin_name, \"\n",
    "#             \"capital: row.capital, \"\n",
    "#             \"population: row.population\"\n",
    "#         \"}\"\n",
    "#     \")\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "# # Add etities (nodes) and relationships (edges) into the graph\n",
    "nodes_as_dict = flatten([list({'id': node.id, 'type': node.type} for node in doc.nodes) for doc in graph_documents])\n",
    "\n",
    "edges_as_dict = flatten([list(\n",
    "    {\n",
    "        'type': rel.type,\n",
    "        'source':{\n",
    "            'id': rel.source.id,\n",
    "            'type': rel.source.type\n",
    "         },\n",
    "        'target': {\n",
    "            'id': rel.target.id,\n",
    "            'type': rel.target.type\n",
    "        }\n",
    "    } for rel in doc.relationships) for doc in graph_documents])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for node in nodes_as_dict:\n",
    "    node_type = re.sub('[^A-Za-z0-9]+', '', node[\"type\"])\n",
    "\n",
    "    query = f\"\"\"\n",
    "    MERGE (n:{node_type} {\"{city_ascii: $id}\" if node['type'] == \"City\" else \"{id: $id}\"})\n",
    "    SET n.type = $type, n.updated = True\n",
    "    \"\"\"\n",
    "    graph.query(query=query, params={\"id\": node[\"id\"], \"type\": node[\"type\"]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for relationship in edges_as_dict:\n",
    "    source_id = relationship[\"source\"][\"id\"]\n",
    "    source_type = re.sub('[^A-Za-z0-9]+', '', relationship[\"source\"][\"type\"])\n",
    "\n",
    "    target_id = relationship[\"target\"][\"id\"]\n",
    "    target_type = re.sub('[^A-Za-z0-9]+', '', relationship[\"target\"][\"type\"])\n",
    "\n",
    "    rel_type = re.sub('[^A-Za-z0-9]+', '', relationship[\"type\"])\n",
    "\n",
    "    print(source_id + \"(\" + source_type + \")\" + \" -[\" + relationship[\"type\"] + \"]-> \" + target_id + \"(\" + target_type + \")\")\n",
    "\n",
    "    condition = \"(a.type = \\\"City\\\" and a.country IS NOT NULL and b.type = \\\"Country\\\" and a.country <> b.id) or (a.type = \\\"Country\\\" and b.type = \\\"City\\\" and b.country IS NOT NULL and a.id <> b.country)\"\n",
    "    query = f\"\"\"\n",
    "        MATCH (a:{source_type} {\"{city_ascii: $source_id}\" if source_type == \"City\" else \"{id: $source_id}\"})\n",
    "        MATCH (b:{target_type} {\"{city_ascii: $target_id}\" if target_type == \"City\" else \"{id: $target_id}\"})\n",
    "        CALL apoc.do.when(\n",
    "            {condition},\n",
    "            'RETURN null',\n",
    "            'MERGE (a)-[r:{rel_type}]->(b) return a, r, b',\n",
    "            {{a: a, b: b}}\n",
    "        )\n",
    "        YIELD value\n",
    "        return value\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    graph.query(\n",
    "        query=query,\n",
    "        params={\n",
    "            \"source_id\": source_id,\n",
    "            \"target_id\": target_id\n",
    "        }\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br><br><br><br>\n",
    "## STEP 4 - Entity matching\n",
    "Since some of the nodes and relationships created are duplicated or have different names to point to the same concept, it is required to execute an \"Entity matching\" phase to reconciliate entities and make them back to just one.\n",
    "\n",
    "For this goal, we'll use **Deepmatcher**, a deep learning-based Entity Matching system that leverages pre-trained language models to improve accuracy in identifying and matching similar entities across different datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4.1 - Prepare dataframe for Deepmatcher\n",
    "First of all, it is necessary to define a Golden Dataset to train Deepmatcher; in this project, the file neo4jCitiesGoldenDataset.csv will do the job. Once read, it'll be split in **train_set**, **val_set** and **test_set** in order to obtain the three **train**, **validation** and **test** datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "goldenDataset = pd.read_csv(\"neo4jCitiesGoldenDataset.csv\", sep=\";\", index_col=\"id\", nrows=10000)\n",
    "train_val_set, test_set = train_test_split(goldenDataset, test_size=0.2, random_state=42)\n",
    "train_set, val_set = train_test_split(train_val_set, test_size=0.25, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:01:52.465126300Z",
     "start_time": "2024-10-24T19:01:52.409066400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_set.to_csv(\".\\\\dataset\\\\train_set.csv\", sep=\",\", index=True, index_label=\"id\")\n",
    "val_set.to_csv(\".\\\\dataset\\\\val_set.csv\", sep=\",\", index=True, index_label=\"id\")\n",
    "test_set.to_csv(\".\\\\dataset\\\\test_set.csv\", sep=\",\", index=True, index_label=\"id\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:01:52.981096Z",
     "start_time": "2024-10-24T19:01:52.928111600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deepmatcher.data.dataset:Rebuilding data cache because: ['One or more data files have been modified.']\n",
      "\n",
      "Reading and processing data from \"dataset\\train_set.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"dataset\\val_set.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"dataset\\test_set.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Building vocabulary\n",
      "0% [######] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n",
      "\n",
      "Computing principal components\n",
      "0% [######] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = \\\n",
    "    dm.data.process(\n",
    "        path='dataset',\n",
    "        train=\"train_set.csv\",\n",
    "        validation=\"val_set.csv\",\n",
    "        test=\"test_set.csv\",\n",
    "        use_magellan_convention=False,\n",
    "        label_attr='is_matching',\n",
    "        left_prefix=\"ltable_\",\n",
    "        right_prefix=\"rtable_\",\n",
    "        ignore_columns=['city_ascii_sim', 'country_sim'],\n",
    "        embeddings_cache_path=\".vector_cache\",\n",
    "        embeddings='fasttext.en.bin'\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:03:03.048306400Z",
     "start_time": "2024-10-24T19:02:31.991917900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "      id ltable_city_ascii ltable_country rtable_city_ascii rtable_country  \\\n1044  27             lagos        nigeria             lagos        nigeria   \n1471  12             cairo          egypt             cairo          egypt   \n2251   7            manila    philippines            manila    philippines   \n2436  35           beijing          china           beining          china   \n2501  19           bangkok       thailand           bangkok       thailand   \n2706  10             seoul    south korea             seoul    south korea   \n3003  33         bangalore          india         mangalore          india   \n3914   5         guangzhou          china         guangzhou          china   \n5593  23      buenos aires      argentina      buenos aires      argentina   \n5971   8          shanghai          china          shanghai          china   \n\n      is_matching  \n1044            1  \n1471            1  \n2251            1  \n2436            1  \n2501            1  \n2706            1  \n3003            1  \n3914            1  \n5593            1  \n5971            1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>ltable_city_ascii</th>\n      <th>ltable_country</th>\n      <th>rtable_city_ascii</th>\n      <th>rtable_country</th>\n      <th>is_matching</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1044</th>\n      <td>27</td>\n      <td>lagos</td>\n      <td>nigeria</td>\n      <td>lagos</td>\n      <td>nigeria</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1471</th>\n      <td>12</td>\n      <td>cairo</td>\n      <td>egypt</td>\n      <td>cairo</td>\n      <td>egypt</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2251</th>\n      <td>7</td>\n      <td>manila</td>\n      <td>philippines</td>\n      <td>manila</td>\n      <td>philippines</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2436</th>\n      <td>35</td>\n      <td>beijing</td>\n      <td>china</td>\n      <td>beining</td>\n      <td>china</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2501</th>\n      <td>19</td>\n      <td>bangkok</td>\n      <td>thailand</td>\n      <td>bangkok</td>\n      <td>thailand</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2706</th>\n      <td>10</td>\n      <td>seoul</td>\n      <td>south korea</td>\n      <td>seoul</td>\n      <td>south korea</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3003</th>\n      <td>33</td>\n      <td>bangalore</td>\n      <td>india</td>\n      <td>mangalore</td>\n      <td>india</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3914</th>\n      <td>5</td>\n      <td>guangzhou</td>\n      <td>china</td>\n      <td>guangzhou</td>\n      <td>china</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5593</th>\n      <td>23</td>\n      <td>buenos aires</td>\n      <td>argentina</td>\n      <td>buenos aires</td>\n      <td>argentina</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5971</th>\n      <td>8</td>\n      <td>shanghai</td>\n      <td>china</td>\n      <td>shanghai</td>\n      <td>china</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table = train.get_raw_table()\n",
    "train_table.loc[train_table[\"is_matching\"] == 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:03:03.136839500Z",
     "start_time": "2024-10-24T19:03:03.056306100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "### Step 4.2 - Train the Matching model\n",
    "Once the datasets are ready, it is possible to train the model with the **train** dataframe and then to fine-tuning it with **validation** dataframe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of trainable parameters: 5056204\n",
      "===>  TRAIN Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabri\\miniconda3\\envs\\deepmatcher\\lib\\site-packages\\torch\\nn\\modules\\module.py:770: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "C:\\Users\\Gabri\\miniconda3\\envs\\deepmatcher\\lib\\site-packages\\torch\\nn\\modules\\module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "C:\\Users\\Gabri\\miniconda3\\envs\\deepmatcher\\lib\\site-packages\\torch\\nn\\modules\\module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "C:\\Users\\Gabri\\miniconda3\\envs\\deepmatcher\\lib\\site-packages\\torch\\nn\\functional.py:2610: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:   53.1 | Load Time:    0.6 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s: 111.61\n",
      "\n",
      "===>  EVAL Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:    5.8 | Load Time:    0.2 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s: 335.00\n",
      "\n",
      "* Best F1: tensor(0.)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:   48.6 | Load Time:    0.4 || F1:  14.29 | Prec:  25.00 | Rec:  10.00 || Ex/s: 122.33\n",
      "\n",
      "===>  EVAL Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:    5.6 | Load Time:    0.1 || F1:  60.00 | Prec:  75.00 | Rec:  50.00 || Ex/s: 347.58\n",
      "\n",
      "* Best F1: tensor(60.)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:   52.0 | Load Time:    0.4 || F1:  53.85 | Prec:  43.75 | Rec:  70.00 || Ex/s: 114.54\n",
      "\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:    6.2 | Load Time:    0.1 || F1:  72.73 | Prec:  80.00 | Rec:  66.67 || Ex/s: 316.45\n",
      "\n",
      "* Best F1: tensor(72.7273)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:   53.0 | Load Time:    0.4 || F1:  80.00 | Prec:  80.00 | Rec:  80.00 || Ex/s: 112.23\n",
      "\n",
      "===>  EVAL Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:    5.7 | Load Time:    0.1 || F1:  66.67 | Prec:  50.00 | Rec: 100.00 || Ex/s: 346.01\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:   54.2 | Load Time:    0.5 || F1:  72.73 | Prec:  66.67 | Rec:  80.00 || Ex/s: 109.71\n",
      "\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    5.6 | Load Time:    0.1 || F1: 100.00 | Prec: 100.00 | Rec: 100.00 || Ex/s: 351.79\n",
      "\n",
      "* Best F1: tensor(100.)\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:   53.6 | Load Time:    0.5 || F1:  76.19 | Prec:  72.73 | Rec:  80.00 || Ex/s: 110.83\n",
      "\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:    6.8 | Load Time:    0.1 || F1: 100.00 | Prec: 100.00 | Rec: 100.00 || Ex/s: 287.15\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:   54.4 | Load Time:    0.5 || F1:  84.21 | Prec:  88.89 | Rec:  80.00 || Ex/s: 109.24\n",
      "\n",
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:    5.9 | Load Time:    0.2 || F1: 100.00 | Prec: 100.00 | Rec: 100.00 || Ex/s: 326.94\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:   54.4 | Load Time:    0.5 || F1:  90.00 | Prec:  90.00 | Rec:  90.00 || Ex/s: 109.34\n",
      "\n",
      "===>  EVAL Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:    6.3 | Load Time:    0.1 || F1: 100.00 | Prec: 100.00 | Rec: 100.00 || Ex/s: 310.13\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:   53.8 | Load Time:    0.5 || F1:  90.91 | Prec:  83.33 | Rec: 100.00 || Ex/s: 110.52\n",
      "\n",
      "===>  EVAL Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:    6.6 | Load Time:    0.2 || F1:  70.59 | Prec:  54.55 | Rec: 100.00 || Ex/s: 296.96\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [███] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:   54.2 | Load Time:    0.5 || F1:  90.91 | Prec:  83.33 | Rec: 100.00 || Ex/s: 109.71\n",
      "\n",
      "===>  EVAL Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:    6.2 | Load Time:    0.2 || F1:  92.31 | Prec:  85.71 | Rec: 100.00 || Ex/s: 312.60\n",
      "\n",
      "---------------------\n",
      "\n",
      "Loading best model...\n",
      "Training done.\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(100.)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = dm.MatchingModel(attr_summarizer='hybrid')\n",
    "model.run_train(\n",
    "    train,\n",
    "    validation,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    best_save_path='hybrid_model.pth',\n",
    "    pos_neg_ratio=6,\n",
    "    log_freq=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:13:03.276379200Z",
     "start_time": "2024-10-24T19:03:03.144838800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "### Step 4.3 - Evaluate the model\n",
    "Now the model is ready to be tested with a **test** dataframe; this step will show how the model performs with never-seen data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    5.3 | Load Time:    0.2 || F1:  94.74 | Prec:  90.00 | Rec: 100.00 || Ex/s: 366.77\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(94.7368)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.run_eval(test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-24T19:13:08.757013500Z",
     "start_time": "2024-10-24T19:13:03.276379200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "### Step 4.4 - Make predictions on real data\n",
    "The model is ready for our final purpose: make the entity matching!\n",
    "To do this, the model needs an **unlabeled dataset** (here we're referencing the **neo4jCitiesUnlabeled.csv** file): this particular dataset doesn't contain any attribute about if the entities are matching or not, so the model can not \"cheat\" and predictions will be genuine!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading and processing data from \"neo4jCitiesUnlabeled.csv\"\n",
      "0% [##############################] 100% | ETA: 00:00:00"
     ]
    }
   ],
   "source": [
    "unlabeled = dm.data.process_unlabeled(\n",
    "    path='neo4jCitiesUnlabeled.csv',\n",
    "    trained_model=model,\n",
    "    ignore_columns=['city_ascii_sim', 'country_sim'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T08:13:17.947536600Z",
     "start_time": "2024-10-25T08:10:26.813997800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>  PREDICT Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [██████████████████████████████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:41:55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time: 2438.5 | Load Time:   77.4 || F1:   0.00 | Prec:   0.00 | Rec:   0.00 || Ex/s:   0.00\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "    match_score ltable_city_ascii ltable_country rtable_city_ascii  \\\nid                                                                   \n1      0.992105             Tokyo          Japan             Tokyo   \n2      0.997656           Jakarta      Indonesia           Jakarta   \n3      0.970387             Delhi          India             Delhi   \n4      0.244146             Delhi          India             Delhi   \n5      0.987772         Guangzhou          China         Guangzhou   \n\n   rtable_country  city_ascii_sim  country_sim  \nid                                              \n1           Japan             1.0     1.000000  \n2       Indonesia             1.0     1.000000  \n3           India             1.0     1.000000  \n4   United States             1.0     0.230769  \n5           China             1.0     1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>match_score</th>\n      <th>ltable_city_ascii</th>\n      <th>ltable_country</th>\n      <th>rtable_city_ascii</th>\n      <th>rtable_country</th>\n      <th>city_ascii_sim</th>\n      <th>country_sim</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.992105</td>\n      <td>Tokyo</td>\n      <td>Japan</td>\n      <td>Tokyo</td>\n      <td>Japan</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.997656</td>\n      <td>Jakarta</td>\n      <td>Indonesia</td>\n      <td>Jakarta</td>\n      <td>Indonesia</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.970387</td>\n      <td>Delhi</td>\n      <td>India</td>\n      <td>Delhi</td>\n      <td>India</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.244146</td>\n      <td>Delhi</td>\n      <td>India</td>\n      <td>Delhi</td>\n      <td>United States</td>\n      <td>1.0</td>\n      <td>0.230769</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.987772</td>\n      <td>Guangzhou</td>\n      <td>China</td>\n      <td>Guangzhou</td>\n      <td>China</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.run_prediction(unlabeled, output_attributes=True)\n",
    "predictions.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T08:56:44.241899600Z",
     "start_time": "2024-10-25T08:14:45.516468900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "predictions.to_csv('predictions/unlabeled_predictions.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-25T08:57:03.038346800Z",
     "start_time": "2024-10-25T08:56:58.766962200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 - Prepare data from Neo4j"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def base_serialization_logic(row, outfile):\n",
    "    serialized_row = \"\"\n",
    "    for column, value in row.items():\n",
    "        if(column != \"type\" and column != \"updated\"):\n",
    "            serialized_row += f\"COL {column} VAL {value} \"\n",
    "    serialized_row = serialized_row.strip()  # Rimuove lo spazio extra finale\n",
    "    outfile.write(serialized_row + \"\\n\")\n",
    "\n",
    "def serialize_csv(input_file, output_file):\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for row in reader:\n",
    "                base_serialization_logic(row, outfile)\n",
    "\n",
    "def serialize_dataframe(df, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for index, row in df.iterrows():\n",
    "            base_serialization_logic(row, outfile)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_file = 'serialized_cities.txt' # Nome del file di output serializzato\n",
    "serialize_dataframe(pd.DataFrame(citiesDf), output_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
